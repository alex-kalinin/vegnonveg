{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VegNonVeg Transfer Learning using Inception V1\n",
    "\n",
    "The goal of this notebook is to use bigdl to retrain last layer of imported inception_v1 model from Caffe, used for Imagenet.  Aims to use transfer learning to classify a dataset of flower images among 5 categories of flowers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T18:21:52.580834Z",
     "start_time": "2017-09-14T18:21:52.568272Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x103e6dc90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify if the spark context was initialized \n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T18:21:53.016289Z",
     "start_time": "2017-09-14T18:21:52.582564Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import all the required packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from os import listdir\n",
    "from os.path import join, basename\n",
    "import struct\n",
    "import json\n",
    "from scipy import misc\n",
    "import datetime as dt\n",
    "\n",
    "from bigdl.nn.layer import *\n",
    "from optparse import OptionParser\n",
    "from bigdl.nn.criterion import *\n",
    "from bigdl.optim.optimizer import *\n",
    "from bigdl.util.common import *\n",
    "from bigdl.dataset.transformer import *\n",
    "from bigdl.nn.initialization_method import *\n",
    "from transformer import *\n",
    "from imagenet import *\n",
    "from transformer import Resize\n",
    "\n",
    "# if you want to train on whole imagenet\n",
    "#from bigdl.dataset import imagenet\n",
    "#%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T18:21:54.334664Z",
     "start_time": "2017-09-14T18:21:54.323485Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper func to read the files from disk\n",
    "def read_local_path(folder, has_label=True):\n",
    "    \"\"\"\n",
    "    :param folder: local directory (str)\n",
    "    :param has_label: does image have label (bool)\n",
    "    :return: list of (image path , label) tuples\n",
    "    \"\"\"\n",
    "    # read directory, create map\n",
    "    dirs = listdir(folder)\n",
    "    # print \"local path: \", folder\n",
    "    # print \"listdir: \", dirs\n",
    "    # create a list of (image path , label) tuples\n",
    "    image_paths = []\n",
    "    #append image path to the label (ex: )\n",
    "    if has_label:\n",
    "        dirs.sort()\n",
    "        for d in dirs:\n",
    "            for f in listdir(join(folder, d)):\n",
    "                image_paths.append((join(join(folder, d), f), dirs.index(d) + 1))\n",
    "    else:\n",
    "        for f in dirs:\n",
    "            image_paths.append((join(folder, f), -1))\n",
    "    return image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T18:21:55.168350Z",
     "start_time": "2017-09-14T18:21:55.152085Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper func to read the files from disk\n",
    "def read_local(sc, folder, normalize=255.0, has_label=True):\n",
    "    \"\"\"\n",
    "    Read images from local directory\n",
    "    :param sc: spark context\n",
    "    :param folder: local directory\n",
    "    :param normalize: normalization value\n",
    "    :param has_label: whether the image folder contains label\n",
    "    :return: RDD of sample\n",
    "    \"\"\"\n",
    "    # read directory, create image paths list\n",
    "    image_paths = read_local_path(folder, has_label)\n",
    "    # print \"BEFORE PARALLELIZATION: \", image_paths\n",
    "    # create rdd\n",
    "    image_paths_rdd = sc.parallelize(image_paths)\n",
    "    # print image_paths_rdd\n",
    "    feature_label_rdd = image_paths_rdd.map(lambda path_label: (misc.imread(path_label[0]), np.array(path_label[1]))) \\\n",
    "        .map(lambda img_label:\n",
    "             (Resize(256, 256)(img_label[0]), img_label[1])) \\\n",
    "        .map(lambda feature_label:\n",
    "             (((feature_label[0] & 0xff) / normalize).astype(\"float32\"), feature_label[1]))\n",
    "    # print \"feature_label_rdd\", feature_label_rdd\n",
    "    return feature_label_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function takes an input, if the input is a list, it insert into index 0 spot, such that the real data starts from index 1. Returns back a dictionary with key being the index and value the list of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T18:21:56.462823Z",
     "start_time": "2017-09-14T18:21:56.455936Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scala_T(input_T):\n",
    "    \"\"\"\n",
    "    Helper function for building Inception layers. Transforms a list of numbers to a dictionary with ascending keys \n",
    "    and 0 appended to the front. Ignores dictionary inputs. \n",
    "    \n",
    "    :param input_T: either list or dict\n",
    "    :return: dictionary with ascending keys and 0 appended to front {0: 0, 1: realdata_1, 2: realdata_2, ...}\n",
    "    \"\"\"    \n",
    "    if type(input_T) is list:\n",
    "        # insert 0 into first index spot, such that the real data starts from index 1\n",
    "        temp = [0]\n",
    "        temp.extend(input_T)\n",
    "        return dict(enumerate(temp))\n",
    "    # if dictionary, return it back\n",
    "    return input_T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are used to create and initiate the inception-v1 model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T18:21:57.724676Z",
     "start_time": "2017-09-14T18:21:57.677480Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question: What is config?\n",
    "def Inception_Layer_v1(input_size, config, name_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Builds the inception-v1 submodule, a local network, that is stacked in the entire architecture when building\n",
    "    the full model.  \n",
    "    \n",
    "    :param input_size: dimensions of input coming into the local network\n",
    "    :param config: ?\n",
    "    :param name_prefix: string naming the layers of the particular local network\n",
    "    :return: concat container object with all of the Sequential layers' ouput concatenated depthwise\n",
    "    \"\"\"        \n",
    "    \n",
    "    '''\n",
    "    Concat is a container who concatenates the output of it's submodules along the provided dimension: all submodules \n",
    "    take the same inputs, and their output is concatenated.\n",
    "    '''\n",
    "    concat = Concat(2)\n",
    "    \n",
    "    \"\"\"\n",
    "    In the above code, we first create a container Sequential. Then add the layers into the container one by one. The \n",
    "    order of the layers in the model is same with the insertion order. \n",
    "    \n",
    "    \"\"\"\n",
    "    conv1 = Sequential()\n",
    "    \n",
    "    #Adding layes to the conv1 model we jus created\n",
    "    \n",
    "    #SpatialConvolution is a module that applies a 2D convolution over an input image.\n",
    "    conv1.add(SpatialConvolution(input_size, config[1][1], 1, 1, 1, 1).set_name(name_prefix + \"1x1\"))\n",
    "    conv1.add(ReLU(True).set_name(name_prefix + \"relu_1x1\"))\n",
    "    concat.add(conv1)\n",
    "    \n",
    "    conv3 = Sequential()\n",
    "    conv3.add(SpatialConvolution(input_size, config[2][1], 1, 1, 1, 1).set_name(name_prefix + \"3x3_reduce\"))\n",
    "    conv3.add(ReLU(True).set_name(name_prefix + \"relu_3x3_reduce\"))\n",
    "    conv3.add(SpatialConvolution(config[2][1], config[2][2], 3, 3, 1, 1, 1, 1).set_name(name_prefix + \"3x3\"))\n",
    "    conv3.add(ReLU(True).set_name(name_prefix + \"relu_3x3\"))\n",
    "    concat.add(conv3)\n",
    "    \n",
    "    \n",
    "    conv5 = Sequential()\n",
    "    conv5.add(SpatialConvolution(input_size,config[3][1], 1, 1, 1, 1).set_name(name_prefix + \"5x5_reduce\"))\n",
    "    conv5.add(ReLU(True).set_name(name_prefix + \"relu_5x5_reduce\"))\n",
    "    conv5.add(SpatialConvolution(config[3][1], config[3][2], 5, 5, 1, 1, 2, 2).set_name(name_prefix + \"5x5\"))\n",
    "    conv5.add(ReLU(True).set_name(name_prefix + \"relu_5x5\"))\n",
    "    concat.add(conv5)\n",
    "    \n",
    "    \n",
    "    pool = Sequential()\n",
    "    pool.add(SpatialMaxPooling(3, 3, 1, 1, 1, 1, to_ceil=True).set_name(name_prefix + \"pool\"))\n",
    "    pool.add(SpatialConvolution(input_size, config[4][1], 1, 1, 1, 1).set_name(name_prefix + \"pool_proj\"))\n",
    "    pool.add(ReLU(True).set_name(name_prefix + \"relu_pool_proj\"))\n",
    "    concat.add(pool).set_name(name_prefix + \"output\")\n",
    "    return concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Inception_v1_Bottleneck(class_num):\n",
    "    model = Sequential()\n",
    "    model.add(SpatialConvolution(3, 64, 7, 7, 2, 2, 3, 3, 1, False).set_name(\"conv1/7x7_s2\"))\n",
    "    model.add(ReLU(True).set_name(\"conv1/relu_7x7\"))\n",
    "    model.add(SpatialMaxPooling(3, 3, 2, 2, to_ceil=True).set_name(\"pool1/3x3_s2\"))\n",
    "    model.add(SpatialCrossMapLRN(5, 0.0001, 0.75).set_name(\"pool1/norm1\"))\n",
    "    model.add(SpatialConvolution(64, 64, 1, 1, 1, 1).set_name(\"conv2/3x3_reduce\"))\n",
    "    model.add(ReLU(True).set_name(\"conv2/relu_3x3_reduce\"))\n",
    "    model.add(SpatialConvolution(64, 192, 3, 3, 1, 1, 1, 1).set_name(\"conv2/3x3\"))\n",
    "    model.add(ReLU(True).set_name(\"conv2/relu_3x3\"))\n",
    "    model.add(SpatialCrossMapLRN(5, 0.0001, 0.75).set_name(\"conv2/norm2\"))\n",
    "    model.add(SpatialMaxPooling(3, 3, 2, 2, to_ceil=True).set_name(\"pool2/3x3_s2\"))\n",
    "    model.add(Inception_Layer_v1(192, scala_T([scala_T([64]), scala_T(\n",
    "         [96, 128]), scala_T([16, 32]), scala_T([32])]), \"inception_3a/\"))\n",
    "    model.add(Inception_Layer_v1(256, scala_T([scala_T([128]), scala_T(\n",
    "         [128, 192]), scala_T([32, 96]), scala_T([64])]), \"inception_3b/\"))\n",
    "    model.add(SpatialMaxPooling(3, 3, 2, 2, to_ceil=True))\n",
    "    model.add(Inception_Layer_v1(480, scala_T([scala_T([192]), scala_T(\n",
    "         [96, 208]), scala_T([16, 48]), scala_T([64])]), \"inception_4a/\"))\n",
    "    model.add(Inception_Layer_v1(512, scala_T([scala_T([160]), scala_T(\n",
    "         [112, 224]), scala_T([24, 64]), scala_T([64])]), \"inception_4b/\"))\n",
    "    model.add(Inception_Layer_v1(512, scala_T([scala_T([128]), scala_T(\n",
    "         [128, 256]), scala_T([24, 64]), scala_T([64])]), \"inception_4c/\"))\n",
    "    model.add(Inception_Layer_v1(512, scala_T([scala_T([112]), scala_T(\n",
    "         [144, 288]), scala_T([32, 64]), scala_T([64])]), \"inception_4d/\"))\n",
    "    model.add(Inception_Layer_v1(528, scala_T([scala_T([256]), scala_T(\n",
    "         [160, 320]), scala_T([32, 128]), scala_T([128])]), \"inception_4e/\"))\n",
    "    model.add(SpatialMaxPooling(3, 3, 2, 2, to_ceil=True))\n",
    "    model.add(Inception_Layer_v1(832, scala_T([scala_T([256]), scala_T(\n",
    "         [160, 320]), scala_T([32, 128]), scala_T([128])]), \"inception_5a/\"))\n",
    "    model.add(Inception_Layer_v1(832, scala_T([scala_T([384]), scala_T(\n",
    "         [192, 384]), scala_T([48, 128]), scala_T([128])]), \"inception_5b/\"))\n",
    "    model.add(SpatialAveragePooling(7, 7, 1, 1).set_name(\"pool5/7x7_s1\"))\n",
    "    model.add(Dropout(0.4).set_name(\"pool5/drop_7x7_s1\"))\n",
    "    model.add(View([1024], num_input_dims=3))\n",
    "    model.reset()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Inception_v1_NoAuxClassifier(class_num):\n",
    "    model = Inception_v1_Bottleneck(class_num)\n",
    "    model.add(Linear(1024, class_num).set_name(\"loss3/classifier_flowers\"))\n",
    "    model.add(LogSoftMax().set_name(\"loss3/loss3\"))\n",
    "    model.reset()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T18:21:59.347703Z",
     "start_time": "2017-09-14T18:21:59.337098Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inception_data(folder, file_type=\"image\", data_type=\"train\", normalize=255.0):\n",
    "    \"\"\"\n",
    "    Builds the entire network using Inception architecture  \n",
    "    \n",
    "    :param class_num: number of categories of classification\n",
    "    :return: entire model architecture \n",
    "    \"\"\"\n",
    "    #Getting the path of our data\n",
    "    path = os.path.join(folder, data_type)\n",
    "    if \"seq\" == file_type:\n",
    "        #return imagenet.read_seq_file(sc, path, normalize) #-- incase if we are trying to read the orig imagenet data\n",
    "        return read_seq_file(sc, path, normalize)\n",
    "    elif \"image\" == file_type:\n",
    "        #return imagenet.read_local(sc, path, normalize)\n",
    "        return read_local(sc, path, normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T16:53:47.651891Z",
     "start_time": "2017-09-14T16:53:47.644313Z"
    }
   },
   "source": [
    "## Creating the Bottleneck model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T18:22:00.747009Z",
     "start_time": "2017-09-14T18:22:00.479356Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initializing BigDL engine\n",
    "init_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T18:22:01.245271Z",
     "start_time": "2017-09-14T18:22:01.241397Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# paths for datasets, saving checkpoints \n",
    "from os import path\n",
    "\n",
    "DATA_ROOT = \"../../../data\"\n",
    "DATA_PATH = DATA_ROOT + \"/vegnonveg-samples\"\n",
    "PROCESSED_PATH = DATA_ROOT + \"/vegnonveg-processed\"\n",
    "checkpoint_path = path.join(DATA_PATH, \"checkpoints\")\n",
    "\n",
    "IMAGE_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T18:22:04.798622Z",
     "start_time": "2017-09-14T18:22:02.435182Z"
    }
   },
   "outputs": [],
   "source": [
    "#providing the no of classes in the dataset to model (5 for flowers)\n",
    "classNum = 5\n",
    "\n",
    "# Instantiating the model the model\n",
    "# inception_model = Inception_v1(classNum)  #-- main inception-v1 model\n",
    "inception_model = Inception_v1_Bottleneck(classNum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Pre-trained Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T18:35:21.545707Z",
     "start_time": "2017-09-14T18:35:21.540522Z"
    }
   },
   "source": [
    "Download the pre-trained 'Inception v1 caffe model' from the link : https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "# path, names of the downlaoded pre-trained caffe models\n",
    "caffe_prototxt = 'bvlc_googlenet.prototxt'\n",
    "caffe_model = 'bvlc_googlenet.caffemodel'\n",
    "\n",
    "if not path.exists(caffe_model):\n",
    "    model_loader = urllib.URLopener()\n",
    "    model_loader.retrieve(\"http://dl.caffe.berkeleyvision.org/bvlc_googlenet.caffemodel\", caffe_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import weights from Caffe Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T18:22:13.685729Z",
     "start_time": "2017-09-14T18:22:10.361537Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading the weights to the BigDL inception model, EXCEPT the weights for the last fc layer (classification layer)\n",
    "model = Model.load_caffe(inception_model, caffe_prototxt, caffe_model, match_all=False, bigdl_type=\"float\")\n",
    "\n",
    "# if we want to export the whole caffe model including definition, this can be used.\n",
    "#model = Model.load_caffe_model(inception_model, caffe_prototxt, caffe_model, match_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process Images\n",
    "\n",
    "0. Load an image\n",
    "0. Crop if not-square to a square shape\n",
    "0. Resize to 224 x 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crop_image(img):\n",
    "    new_dim = min(img.width, img.height)\n",
    "    x = (img.width - new_dim) / 2\n",
    "    y = (img.height - new_dim) / 2\n",
    "    cropped = img.crop((x, y, x + new_dim, y + new_dim))\n",
    "    return cropped\n",
    "\n",
    "# transform_input = Transformer([ChannelNormalizer(0.485, 0.456, 0.406, 0.229, 0.224, 0.225),\n",
    "#                                TransposeToTensor(False)])\n",
    "\n",
    "def preprocess_images(image_dir, out_dir):\n",
    "    file_names = sorted(os.listdir(image_dir))\n",
    "    for file_name in file_names:\n",
    "        image_path = path.join(image_dir, file_name)\n",
    "        input_img = Image.open(image_path)\n",
    "        cropped = crop_image(input_img)\n",
    "        resized = cropped.resize((IMAGE_SIZE, IMAGE_SIZE), Image.ANTIALIAS)\n",
    "        out_path = path.join(out_dir, file_name) + '.jpg'\n",
    "        resized.save(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocess_images(DATA_PATH, PROCESSED_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Bottlenecks for Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_images(image_dir):\n",
    "    file_names = sorted(os.listdir(image_dir))\n",
    "    images = np.zeros((len(file_names), IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    for index, file_name in enumerate(file_names):\n",
    "        file_path = path.join(image_dir, file_name)\n",
    "        input_img = Image.open(file_path).convert('RGB')\n",
    "        img_np = np.array(input_img)\n",
    "        assert img_np.shape[0] == 224, file_path\n",
    "        images[index] = img_np\n",
    "    return file_names, images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2611, (2611, 224, 224, 3))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names, imgs = load_images(PROCESSED_PATH)\n",
    "len(file_names), imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform_input = Transformer([TransposeToTensor(False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# img = img_batch[0]\n",
    "# print(img.shape)\n",
    "# img_t = transform_input(img)\n",
    "# print(img_t.shape)\n",
    "# # rdd_images = sc.parallelize([Sample.from_ndarray(img, np.array(0))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_list = []\n",
    "batch_size = 128\n",
    "for start in range(0, imgs.shape[0], batch_size):\n",
    "    img_batch = imgs[start : start + batch_size]\n",
    "    rdd_img = sc.parallelize(img_batch)\n",
    "    rdd_sample = rdd_img.map(lambda img: Sample.from_ndarray(transform_input(img), np.array(0)))    \n",
    "    preds = model.predict(rdd_sample)\n",
    "    p = preds.collect()\n",
    "    pred_list.extend(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2611"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = pd.read_csv(DATA_ROOT + '/vegnonveg-samples_labels.csv')\n",
    "labels = labels.sort_values(by='obs_uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a, b in zip(file_names, labels['obs_uid'].tolist()):\n",
    "#     assert a == b + \".jpg\", \"{} = {}\".format(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    'bottleneck_values': pred_list,\n",
    "    'labels': labels['item_name'].tolist()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(data, open(DATA_ROOT + \"/bottlenecks_with_labels.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test NN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pickle.load(open(DATA_ROOT + \"/bottlenecks_with_labels.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2088, 2088, 523, 523)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, train_labels, test_labels = train_test_split(data['bottleneck_values'], data['labels'], test_size=0.2, random_state=101)\n",
    "len(x_train), len(train_labels), len(x_test), len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(256,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59.2 s, sys: 6.51 s, total: 1min 5s\n",
      "Wall time: 33.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.56415094,  0.56190476,  0.6088632 ,  0.57198444])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "cross_val_score(clf, x_train, train_labels, cv=4, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Other classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31 s, sys: 153 ms, total: 31.2 s\n",
      "Wall time: 30.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.5754717 ,  0.56380952,  0.56069364,  0.54669261])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "cross_val_score(LogisticRegression(), x_train, train_labels, cv=4, scoring='accuracy')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
